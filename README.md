# Visual Spatial Relations Learning with Contrastive Embeddings

**Apprentissage de Relations Spatiales par Embeddings Contrastifs**

> ModÃ©lisation SystÃ¨mes Intelligents  - ModÃ©lisation des relations spatiales Ã  partir de donnÃ©es raster . 

##

 ğŸ“‹ RÃ©sumÃ©

Ce projet explore l'apprentissage par approche contrastive pour la dÃ©tection de relations spatiales entre objets dans des images. En utilisant le dataset Visual Relationship Detection (VRD), nous dÃ©veloppons des modÃ¨les dual-encodeurs qui apprennent Ã  aligner les reprÃ©sentations visuelles et gÃ©omÃ©triques des relations spatiales.

**RÃ©sultats clÃ©s** :
- **62.40% accuracy** sur VRD (10 classes spatiales)
- Architecture dual-encoder : ResNet-50 (visuel) + MLP (spatial)
- GÃ©nÃ©ralisation cross-dataset sur PSG (Recall@10: 84.26%)

---

## ğŸ¯ ProblÃ©matique

### Objectif
DÃ©velopper un modÃ¨le capable de comprendre et classifier les **relations spatiales** entre objets dÃ©tectÃ©s dans une image, en exploitant Ã  la fois les informations visuelles et gÃ©omÃ©triques.

### Relations Spatiales Cibles (10 classes)

| CatÃ©gorie | Relations |
|-----------|-----------|
| **Verticales** | `above`, `below`, `on`, `under` |
| **Horizontales** | `left of`, `right of` |
| **ProximitÃ©** | `near`, `next to` |
| **Containment** | `inside`, `outside` |

### Challenges
- **AmbiguÃ¯tÃ© sÃ©mantique** : FrontiÃ¨res floues entre relations (near vs next to)
- **DÃ©sÃ©quilibre des classes** : Ratio 344:1 (on vs outside)
- **DÃ©pendance au contexte** : InterprÃ©tation variable selon Ã©chelle
- **GÃ©nÃ©ralisation cross-dataset** : Transfer learning VRD â†’ PSG

---

## ğŸ“Š Dataset: Visual Relationship Detection (VRD)

### Vue d'Ensemble

Le **Visual Relationship Detection (VRD)** dataset, introduit par Lu et al. (ECCV 2016), est un benchmark de rÃ©fÃ©rence pour la comprÃ©hension des relations visuelles entre objets.

**Statistiques ComplÃ¨tes** :
- **5,000 images** (4,000 train + 1,000 test), issues de MS-COCO et ImageNet
- **100 catÃ©gories d'objets** (person, car, table, chair, dog, etc.)
- **70 types de prÃ©dicats** (relations visuelles)
- **37,993 triplets annotÃ©s** au format $\langle$sujet, prÃ©dicat, objet$\rangle$
- Chaque triplet contient les bounding boxes des objets

### Types de Relations

Le dataset VRD couvre **70 prÃ©dicats** rÃ©partis en 4 catÃ©gories :

1. **Relations Spatiales** (notre focus) :
   - Verticales : on, above, below, under, over
   - Horizontales : left of, right of
   - ProximitÃ© : near, next to, beside
   - Containment : inside, outside, in

2. **Relations d'Action** :
   - Interactions : riding, holding, carrying, wearing, eating, drinking
   - Manipulations : touching, pushing, pulling, kicking

3. **Relations Comparatives** :
   - Tailles : taller than, shorter than, bigger than
   - Ã‚ges : older than, younger than

4. **Autres Relations** :
   - PropriÃ©tÃ©s : made of, has, part of, attached to
   - Ã‰tats : watching, looking at, playing with

### Notre Focus : 10 Relations Spatiales

Pour ce projet, nous nous concentrons sur **10 relations spatiales pures** :

| CatÃ©gorie | Relations | Support Test | % Total |
|-----------|-----------|--------------|---------|
| **Verticales** | on, above, below, under | 3,255 | 73.8% |
| **Horizontales** | left of, right of | 221 | 5.0% |
| **ProximitÃ©** | near, next to | 911 | 20.7% |
| **Containment** | inside, outside | 22 | 0.5% |
| **TOTAL** | **10 classes** | **4,409** | **100%** |

### Distribution des Classes (Test Set)

---

## ğŸ—ï¸ Architecture

### Dual-Encoder Contrastive Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT: Image + BBoxes                    â”‚
â”‚          (Subject BBox, Object BBox, Image Context)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VISUAL ENCODER â”‚        â”‚SPATIAL ENCODERâ”‚
â”‚   (ResNet-50)  â”‚        â”‚     (MLP)     â”‚
â”‚                â”‚        â”‚               â”‚
â”‚ Crop Subject   â”‚        â”‚ Geometric Vec â”‚
â”‚ Crop Object    â”‚        â”‚   (8D)        â”‚
â”‚ Features 2048D â”‚        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚      â†“         â”‚        â”‚ â”‚ dx, dy    â”‚ â”‚
â”‚ Concatenate    â”‚        â”‚ â”‚ distanceÂ² â”‚ â”‚
â”‚   4096D        â”‚        â”‚ â”‚ angle     â”‚ â”‚
â”‚      â†“         â”‚        â”‚ â”‚ areas     â”‚ â”‚
â”‚  Projection    â”‚        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  4096â†’1024â†’128 â”‚        â”‚ 8â†’64â†’128â†’128  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚   Embedding 128D       â”‚   Embedding 128D
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  CONTRASTIVE  â”‚
          â”‚     LOSS      â”‚
          â”‚  (InfoNCE /   â”‚
          â”‚   SupCon)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture DÃ©taillÃ©e

Notre approche **dual-encoder** apprend simultanÃ©ment deux reprÃ©sentations complÃ©mentaires :

#### 1. Visual Encoder (ResNet-50)

**Objectif** : Capturer les caractÃ©ristiques visuelles des objets et leur contexte.

**Pipeline** :
1. **Extraction crops** : Deux crops 128Ã—128 (sujet et objet) depuis l'image originale
2. **Feature extraction** : ResNet-50 prÃ©-entraÃ®nÃ© sur ImageNet (frozen)
   - Sujet : 2048D features
   - Objet : 2048D features
3. **Concatenation** : 4096D feature vector
4. **Projection MLP** : 4096D â†’ 1024D â†’ 128D
   - Reduce dimensionnalitÃ©
   - Dropout 0.3 pour rÃ©gularisation
   - ReLU activations

**Pourquoi frozen backbone ?**
- Transfer learning depuis ImageNet (features gÃ©nÃ©riques)
- Ã‰vite overfitting (dataset VRD relativement petit)
- RÃ©duit coÃ»t computationnel

#### 2. Spatial Encoder (MLP)

**Objectif** : Encoder la gÃ©omÃ©trie pure de la relation spatiale.

**Pipeline** :
1. **Input** : Vecteur spatial 8D (voir section suivante)
2. **MLP 3 couches** : 8D â†’ 64D â†’ 128D â†’ 128D
   - Expansion puis stabilisation
   - Dropout 0.2
   - ReLU activations
3. **Output** : Embedding spatial 128D

**Design clÃ©** : Architecture lÃ©gÃ¨re car gÃ©omÃ©trie = information structurÃ©e

#### 3. Contrastive Loss (InfoNCE)

**Formulation** :
```
L = -log(exp(sim(z_v, z_s) / Ï„) / Î£ exp(sim(z_v, z_s') / Ï„))
```

OÃ¹ :
- `z_v` : embedding visuel (128D)
- `z_s` : embedding spatial (128D)
- `sim()` : similaritÃ© cosine (aprÃ¨s L2 normalization)
- `Ï„ = 0.07` : tempÃ©rature

**Objectif** : Aligner embeddings visuels et spatiaux correspondants dans l'espace latent.

---

### Vecteur GÃ©omÃ©trique (8D)

```python
spatial_vector = [
    dx_norm,      # DÃ©placement horizontal normalisÃ©
    dy_norm,      # DÃ©placement vertical normalisÃ©  
    distanceÂ²,    # DistanceÂ² normalisÃ©e par diagonal
    sin(Î¸),       # Angle direction (sin)
    cos(Î¸),       # Angle direction (cos)
    log(area_s),  # Log aire sujet
    log(area_o),  # Log aire objet
    iou           # Intersection over Union
]
```

---

## ğŸ“Š Protocole ExpÃ©rimental

### Dataset: Visual Relationship Detection (VRD)

**Distribution** :
- Total: 5,000 images, 37,993 relations annotÃ©es
- **Train**: 4,000 images
- **Validation**: 1,000 images  
- **Test**: 4,409 Ã©chantillons (10 classes spatiales)

**Filtrage** : Relations spatiales uniquement (10 classes)

### HyperparamÃ¨tres

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Batch size | 16 |
| Learning rate | 1e-4 |
| Optimizer | Adam |
| Epochs max | 50 |
| Early stopping | 3 epochs patience |
| Embedding dim | 128D (visuel + spatial) |
| Image size | 128Ã—128 |

### RÃ©gularisation

- **Backbone frozen**: Transfer learning depuis ImageNet
- **Dropout**: 0.3 (visual), 0.2 (spatial)
- **Data augmentation**: RandomHorizontalFlip, ColorJitter (train)
- **Early stopping**: Sur validation accuracy

---

## ğŸ”¬ ExpÃ©riences MenÃ©es

### Vue d'Ensemble

| Exp | Architecture | Loss | Sampling | Classes | Val Acc | Test Acc |
|-----|--------------|------|----------|---------|---------|----------|
| **#1** | ResNet-18 | InfoNCE | Random | 10 VRD | 48.01% | **61.67%** |
| **#2** | ResNet-50 | InfoNCE | Balanced | 10 VRD | 56.88% | **62.40%** â­ |
| #3 | EfficientNet-B0 | InfoNCE | Random | 10 VRD | 51.62% | 56.54% |
| #4 | ResNet-18 | SupCon | Balanced | 10 VRD | - | (En cours) |
| #5 | ResNet-18 | SupCon | Balanced | 6 merged | 10% | Ã‰chec |
| #6 | ResNet-18 | InfoNCE | Random | 5 top | 44.3% | - |

---

## ğŸ“ˆ RÃ©sultats DÃ©taillÃ©s

### Exp #1: Baseline (ResNet-18 + InfoNCE)

**Configuration** :
- Architecture: ResNet-18 (11M params, frozen)
- Loss: InfoNCE (self-supervised contrastive)
- Sampling: Random
- Epochs: 15 (early stopped)

**Training Curve** :

```
Epoch | Train Loss | Val Loss | Val Acc
------|------------|----------|--------
1     | 2.4303     | 2.1613   | 25.53%
5     | 1.8522     | 1.7402   | 39.20%
10    | 1.6030     | 1.5756   | 44.14%
15    | 1.4701     | 1.4463   | 48.01%
```

![Courbes Loss Exp #1](assets/exp1_loss_curve.png)
*Courbes d'entraÃ®nement Exp #1 (ResNet-18): Convergence Ã  epoch 15*

**RÃ©sultats Test** :
- **Accuracy**: 61.67%
- Visual seul: 53.14%
- Spatial seul: 58.15%
- **Fusion (V+S)**: 61.67%

**Performance par Classe** :

| Classe | Precision | Recall | F1-Score | Support |
|--------|-----------|--------|----------|---------|
| on | 0.69 | 0.91 | 0.78 | 1867 âœ… |
| above | 0.71 | 0.68 | 0.69 | 714 âœ… |
| under | 0.44 | 0.59 | 0.51 | 431 âš ï¸ |
| next to | 0.41 | 0.44 | 0.42 | 563 âš ï¸ |
| below | 0.42 | 0.10 | 0.16 | 243 âŒ |
| left of | 1.00 | 0.02 | 0.03 | 116 âŒ |
| right of | 0.00 | 0.00 | 0.00 | 105 âŒ |
| near | 0.43 | 0.03 | 0.05 | 348 âŒ |
| inside | 0.00 | 0.00 | 0.00 | 19 âŒ |
| outside | 0.00 | 0.00 | 0.00 | 3 âŒ |

---

### Exp #2: Meilleur ModÃ¨le (ResNet-50 + InfoNCE) â­

**Configuration** :
- Architecture: ResNet-50 (25M params, frozen)
- Loss: InfoNCE (contrastive)
- Sampling: Balanced batch sampler
- Epochs: 23 (early stopped)

**Training Curve** :

```
Epoch | Train Loss | Val Loss | Val Acc
------|------------|----------|--------
1     | 2.1788     | 1.8121   | 36.50%
5     | 1.4200     | 1.6035   | 47.39%
10    | 1.0760     | 1.4436   | 52.48%
15    | 0.8858     | 1.3034   | 55.94%
20    | 0.7634     | 1.2958   | 57.79%
23    | 0.6615     | 1.4205   | 56.88%
```

![Courbes Loss Exp #2](assets/exp2_loss_curve.png)
*Courbes d'entraÃ®nement Exp #2 (ResNet-50): Convergence plus stable, loss finale plus basse*

**RÃ©sultats Test** :
- **Accuracy**: 62.40%
- Visual seul: 55.50%
- Spatial seul: 58.74%
- **Fusion (V+S)**: 62.40% ğŸ†

**Performance par Classe** :

| Classe | Precision | Recall | F1-Score | Support |
|--------|-----------|--------|----------|---------|
| on | 0.70 | 0.91 | 0.79 | 1867 âœ… |
| above | 0.73 | 0.69 | 0.71 | 714 âœ… |
| under | 0.46 | 0.60 | 0.52 | 431 âœ… |
| next to | 0.40 | 0.44 | 0.42 | 563 âš ï¸ |
| below | 0.49 | 0.14 | 0.21 | 243 âŒ |
| left of | 0.50 | 0.03 | 0.06 | 116 âŒ |
| right of | 0.50 | 0.02 | 0.04 | 105 âŒ |
| near | 0.33 | 0.02 | 0.04 | 348 âŒ |
| inside | 0.00 | 0.00 | 0.00 | 19 âŒ |
| outside | 0.00 | 0.00 | 0.00 | 3 âŒ |

**Matrice de Confusion** :

![Confusion Matrix Exp #2](assets/confusion_matrix_exp2.png)
*Matrice de confusion (10Ã—10): Excellente performance sur "on" et "above", confusion sur classes rares*

**Observations** :
- **Diagonale forte** : on (1700/1867), above (492/714)
- **Confusion principale** : Classes rares â†’ prÃ©dites comme "on" (biais)
- **Pattern** : near â†” next to (ambiguÃ¯tÃ© sÃ©mantique)

**AmÃ©lioration vs Exp #1** :
- Accuracy: +0.73 points
- F1-Score moyen pondÃ©rÃ©: +0.01
- Meilleure convergence (loss plus bas)
- LÃ©gÃ¨re amÃ©lioration sur classes rares

---

### Exp #3: EfficientNet-B0

**RÃ©sultats** :
- Test Accuracy: 56.54%
- Architecture plus compacte mais features moins riches
- Training plus rapide mais performances infÃ©rieures

---

### Autres ExpÃ©riences

**Exp #4** : Supervised Contrastive + Balanced sampling (en cours)

**Exp #5** : Classes fusionnÃ©es (6 classes)
- Tentative de fusion sÃ©mantique: "vertical", "horizontal", "contact"...
- **Ã‰chec**: 10% accuracy (confusion totale)
- LeÃ§on: La granularitÃ© originale est nÃ©cessaire

**Exp #6** : Top-5 classes (on, above, under, next to, below)
- 44.3% accuracy sur 5 classes sÃ©lectionnÃ©es
- Objectif: RÃ©duire dÃ©sÃ©quilibre
- RÃ©sultat mitigÃ©: Classes rares toujours problÃ©matiques

---

## ğŸŒ GÃ©nÃ©ralisation Cross-Dataset (PSG)

### Transfer Learning VRD â†’ PSG

Pour tester la capacitÃ© de gÃ©nÃ©ralisation, nous avons Ã©valuÃ© le modÃ¨le VRD sur le dataset **Panoptic Scene Graph (PSG)** :

**Protocole** :
1. Utiliser les embeddings prÃ©-entraÃ®nÃ©s (Exp #2)
2. EntraÃ®ner un classifier SVM sur embeddings PSG
3. Ã‰valuer avec Recall@K metrics

**RÃ©sultats (56 classes PSG complÃ¨tes)** :
```
Accuracy (top-1):  35.27%
Recall@1:          35.27%
Recall@5:          72.87% âœ…
Recall@10:         84.26% âœ…
```

**RÃ©sultats (14 classes spatiales PSG)** :
```
Accuracy:          43.11%

Classes performantes:
- over:        69% F1
- on:          51% F1
- beside:      47% F1
- in front of: 45% F1
```

**Analyse** :
- âœ… Bonne gÃ©nÃ©ralisation sur relations spatiales pures
- âŒ Faible performance sur actions (eating, holding...)
- âœ… Recall@10 excellent (84%) â†’ Bon pour retrieval
- Limitation: Features spatiales ne capturent pas dÃ©tails visuels d'actions

---

## ğŸ’¡ Analyse et Insights

### Forces du ModÃ¨le

1. **Relations dominantes** : Excellent sur "on", "above" (F1 > 70%)
2. **Fusion efficace** : Visual + Spatial > chacun sÃ©parÃ©ment
3. **Transfer learning** : Generalise aux relations spatiales cross-dataset
4. **Embeddings riches** : Recall@10 = 84% sur PSG

### Limitations

1. **Classes rares** : "outside", "inside", "right of" (< 20 samples) â†’ 0% F1
2. **AmbiguÃ¯tÃ© sÃ©mantique** : "near" vs "next to" difficile Ã  distinguer
3. **DÃ©sÃ©quilibre** : "on" domine (42%) â†’ biais de prÃ©diction
4. **Actions** : Features gÃ©omÃ©triques insuffisantes pour actions visuelles

### Insights ClÃ©s

| Observation | Implication |
|-------------|-------------|
| ResNet-50 > ResNet-18 | Features plus riches amÃ©liorent lÃ©gÃ¨rement |
| Spatial Encoder performant | GÃ©omÃ©trie capture bien relations pures |
| Classes rare problem | Besoin oversampling ou hard negative mining |
| Cross-dataset OK pour spatial | Embeddings spatiaux gÃ©nÃ©ralisent |

---

## ğŸš€ Utilisation

### Installation

```bash
# Clone repository
git clone https://github.com/[your-username]/MSI-Projet_Spatial_Relations.git
cd MSI-Projet_Spatial_Relations

# Install dependencies
pip install -r requirements.txt
```

### Dataset VRD

TÃ©lÃ©charger le [VRD dataset](https://cs.stanford.edu/people/ranjaykrishna/vrd/) et placer dans `vrd/`:

```
vrd/
â”œâ”€â”€ images/
â”œâ”€â”€ annotations_train.json
â””â”€â”€ annotations_test.json
```

### EntraÃ®nement

```bash
# Train baseline
python -m src.train

# Result will be saved in checkpoints/exp_YYYYMMDD_HHMMSS/
```

### Ã‰valuation

```bash
# Evaluate checkpoint
python -m src.evaluate checkpoints/exp_YYYYMMDD_HHMMSS

# Cross-dataset evaluation (PSG)
python scripts/evaluate_psg_full56.py
```

---

## ğŸ“ Structure du Projet

```
MSI-Projet_Spatial_Relations/
â”‚
â”œâ”€â”€ README.md                    â† Ce fichier
â”œâ”€â”€ requirements.txt             â† DÃ©pendances Python
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/                         â† Code source principal
â”‚   â”œâ”€â”€ config.py                â† Configuration systÃ¨me
â”‚   â”œâ”€â”€ dataset.py               â† DataLoader VRD
â”‚   â”œâ”€â”€ model.py                 â† Architectures encodeurs
â”‚   â”œâ”€â”€ train.py                 â† Pipeline entraÃ®nement
â”‚   â”œâ”€â”€ evaluate.py              â† Ã‰valuation checkpoints
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ geometry.py          â† Calculs gÃ©omÃ©triques
â”‚
â”œâ”€â”€ scripts/                     â† Scripts utilitaires
â”‚   â”œâ”€â”€ analyze_dataset.py
â”‚   â”œâ”€â”€ visualize_vrd.py
â”‚   â””â”€â”€ evaluate_psg_full56.py   â† Ã‰val cross-dataset
â”‚
â”œâ”€â”€ experiments/                 â† Documentation expÃ©riences
â”‚   â”œâ”€â”€ README.md                â† Vue d'ensemble rÃ©sultats
â”‚   â”œâ”€â”€ EXP2_CONFIG.md
â”‚   â””â”€â”€ results/
â”‚       â””â”€â”€ psg_evaluation.txt
â”‚
â””â”€â”€ assets/                      â† Images README
    â””â”€â”€ loss_curve.png
```

---

## ğŸ“š RÃ©fÃ©rences

### Datasets
- **VRD**: Lu et al. ["Visual Relationship Detection with Language Priors"](https://cs.stanford.edu/people/ranjaykrishna/vrd/), ECCV 2016
- **PSG**: Yang et al. ["Panoptic Scene Graph Generation"](https://psgdataset.org/), ECCV 2022

### MÃ©thodes
- **InfoNCE Loss**: Oord et al. "Representation Learning with Contrastive Predictive Coding", 2018
- **Supervised Contrastive**: Khosla et al. "Supervised Contrastive Learning", NeurIPS 2020
- **ResNet**: He et al. "Deep Residual Learning for Image Recognition", CVPR 2016

### Frameworks
- PyTorch 2.0
- torchvision (pre-trained models)
- scikit-learn (SVM evaluation)

---

## ğŸ‘¨â€ğŸ“ Contexte AcadÃ©mique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre d'un Master en Vision par Ordinateur, avec un focus sur :
- L'apprentissage contrastif pour la vision
- La comprÃ©hension des relations spatiales
- La gÃ©nÃ©ralisation cross-dataset
- L'Ã©valuation scientifique rigoureuse

**CompÃ©tences ** :
- ImplÃ©mentation PyTorch avancÃ©e
- Conception d'architecture dual-encoder
- ExpÃ©rimentation systÃ©matique (6 configurations)
- Analyse quantitative et qualitative
- Transfer learning cross-dataset

---

## ğŸ“§ Contact

El haj Samitt Ebou 
el-haj-samitt.ebou@etu.u-paris.fr 


